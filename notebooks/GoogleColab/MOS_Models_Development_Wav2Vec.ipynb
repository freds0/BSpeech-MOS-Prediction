{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MOS_Models_Development_Wav2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgLwghueQL5J"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzinmPvvQQk7",
        "outputId": "19c73bbb-108e-483f-e687-5222479cd4ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat May 14 01:43:21 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    33W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2Wxt1UiQUmX",
        "outputId": "c01b828b-ff55-4d9f-98ff-a26dbb771df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lns6I_LVakih",
        "outputId": "5f2264bb-735d-4a6d-d23a-b82df7625931"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (22.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1eLVEr6dvycpzCBsW7qcpYE5LoSahgz_W/view?usp=sharing\n",
        "import gdown, os\n",
        "os.chdir('/content')\n",
        "id = '1eLVEr6dvycpzCBsW7qcpYE5LoSahgz_W'\n",
        "url = \"https://drive.google.com/uc?id={}\".format(id)\n",
        "output = \"VCC2018_MOS_preprocessed.tar.bz\"\n",
        "\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "_6K3XuY-Qddc",
        "outputId": "0d63a3e4-77ad-4642-fad8-a87f97f2c593"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eLVEr6dvycpzCBsW7qcpYE5LoSahgz_W\n",
            "To: /content/VCC2018_MOS_preprocessed.tar.bz\n",
            "100%|██████████| 1.32G/1.32G [00:13<00:00, 97.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'VCC2018_MOS_preprocessed.tar.bz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "\n",
        "tar = tarfile.open(output, \"r:bz2\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "metadata": {
        "id": "J3OpKLRdTp1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/VCC2018_MOS_preprocessed/mos_list.txt', header=None)\n",
        "df[0] = '/content/VCC2018_MOS_preprocessed/wav/' + df[0] \n",
        "df.columns = ['filepath', 'score']\n",
        "df.head()"
      ],
      "metadata": {
        "id": "IojfNM4YT-7W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a6bfb212-301e-4f8b-c11f-6971d8788ba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            filepath  score\n",
              "0  /content/VCC2018_MOS_preprocessed/wav/N14_VCC2...   1.25\n",
              "1  /content/VCC2018_MOS_preprocessed/wav/N14_VCC2...   2.75\n",
              "2  /content/VCC2018_MOS_preprocessed/wav/N14_VCC2...   3.50\n",
              "3  /content/VCC2018_MOS_preprocessed/wav/N14_VCC2...   1.75\n",
              "4  /content/VCC2018_MOS_preprocessed/wav/N14_VCC2...   4.00"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f312318a-58d9-428c-8a32-f6beb30cea52\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filepath</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/VCC2018_MOS_preprocessed/wav/N14_VCC2...</td>\n",
              "      <td>1.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/VCC2018_MOS_preprocessed/wav/N14_VCC2...</td>\n",
              "      <td>2.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/VCC2018_MOS_preprocessed/wav/N14_VCC2...</td>\n",
              "      <td>3.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/VCC2018_MOS_preprocessed/wav/N14_VCC2...</td>\n",
              "      <td>1.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/VCC2018_MOS_preprocessed/wav/N14_VCC2...</td>\n",
              "      <td>4.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f312318a-58d9-428c-8a32-f6beb30cea52')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f312318a-58d9-428c-8a32-f6beb30cea52 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f312318a-58d9-428c-8a32-f6beb30cea52');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(df['score'].unique())\n",
        "num_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSxnpABrXAYC",
        "outputId": "22bee514-8f31-45e5-bec3-132070228478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df['score'])\n",
        "df['score'] = le.transform(df['score'])"
      ],
      "metadata": {
        "id": "N7QU-FQ7awmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['score']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHFowCWMb1j_",
        "outputId": "9c075648-ee34-4a6b-d171-2787e56b7a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0         1\n",
              "1         7\n",
              "2        11\n",
              "3         3\n",
              "4        14\n",
              "         ..\n",
              "22355    20\n",
              "22356    20\n",
              "22357    20\n",
              "22358    20\n",
              "22359    20\n",
              "Name: score, Length: 22360, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['score'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mx0Yjz0ZSXQ",
        "outputId": "5b8ab2ea-883d-44d6-bc7f-837cc943370c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22360,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsENyxyOWRUU",
        "outputId": "819890c6-2a09-4562-d299-82d14ef98234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22360"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torchaudio\n",
        "data = []\n",
        "\n",
        "wavs_folder = '/content/VCC2018_MOS_preprocessed/wav/'\n",
        "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    #filepath = os.path.join(wavs_folder, row['path'])\n",
        "    filepath = row['filepath']\n",
        "    score = row['score']\n",
        "    try:\n",
        "        # There are some broken files\n",
        "        s = torchaudio.load(filepath)\n",
        "        data.append({\n",
        "            # \"name\": name,\n",
        "            \"filepath\": filepath,\n",
        "            \"score\": score\n",
        "        })\n",
        "    except Exception as e:\n",
        "        #print(str(filepath), e)\n",
        "        pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIFi3wOir8wo",
        "outputId": "a51185a9-82d5-4c50-e5d1-29a3d1180a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22360/22360 [00:23<00:00, 958.31it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "U72Aj9AJsjNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data = train_test_split(data, test_size=0.10) "
      ],
      "metadata": {
        "id": "E0sA0gu_VwRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories_train = train_data['score'].to_list()\n",
        "audio_names_train = train_data['filepath'].to_list()\n",
        "\n",
        "categories_test = test_data['score'].to_list()\n",
        "audio_names_test = test_data['filepath'].to_list()"
      ],
      "metadata": {
        "id": "fKRuZ0rGXCm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get all unique categories\n",
        "categories_types = np.sort(df['score'].unique())\n",
        "categories_types, categories_types.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_a84LRDaRNS",
        "outputId": "b52c1d96-4ee5-4c7c-a2ff-76fd4170c77b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "        17, 18, 19, 20]), (21,))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we will do saving melspectrograms just to skip the process of loading, getting specs too often\n",
        "def normalize(spec, eps=1e-6):\n",
        "    mean = spec.mean()\n",
        "    std = spec.std()\n",
        "    spec_norm = (spec - mean) / (std + eps)\n",
        "    spec_min, spec_max = spec_norm.min(), spec_norm.max()\n",
        "    spec_scaled = 255 * (spec_norm - spec_min) / (spec_max - spec_min)\n",
        "    spec_scaled = spec_scaled.astype(np.uint8)\n",
        "    return spec_scaled\n",
        "\n",
        "def get_transform_specs(audio_names, data_dir='./'):\n",
        "    # get audio path\n",
        "    specs = []\n",
        "    for audio_name in audio_names:\n",
        "      audio_path = os.path.join(data_dir, audio_name)\n",
        "        \n",
        "      # load audio and get its melspectrogram\n",
        "      audio_wave, sr = librosa.load(audio_path, None)\n",
        "      if audio_wave.shape[0]<5*sr:\n",
        "        audio_wave = np.pad(audio_wave, int(np.ceil((5*sr-audio_wave.shape[0])/2)), mode='reflect')\n",
        "      else:\n",
        "        audio_wave = audio_wave[:5*sr]\n",
        "\n",
        "      mel_spec = librosa.feature.melspectrogram(audio_wave, sr=sr, n_fft=2048, hop_length=512, n_mels=128, fmin=20, fmax=8300)\n",
        "      \n",
        "      mel_spec = librosa.power_to_db(mel_spec, top_db=80)\n",
        "      mel_spec = normalize(mel_spec)\n",
        "      specs.append(mel_spec[np.newaxis,...])\n",
        "\n",
        "    return specs"
      ],
      "metadata": {
        "id": "nU55fpTvaYsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mos_to_id = dict((mos,id) for id, mos in enumerate(categories_types))"
      ],
      "metadata": {
        "id": "u3cmoFDDc6EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if we don't do preprocessing - saving specs - use this dataset \n",
        "class VCC2018MOSWavDatasetLoad(Dataset):\n",
        "    def __init__(self, audio_names: list, labels: list):\n",
        "        self.audio_names = audio_names\n",
        "        self.labels = labels\n",
        "\n",
        "        #self.label_to_id = dict((mos,id) for id, mos in enumerate(labels))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.audio_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.audio_names[idx]\n",
        "        waveform, sample_rate = torchaudio.load(filename)\n",
        "        #target = self.label_to_id[self.labels[idx]]\n",
        "        target = self.labels[idx]\n",
        "        \n",
        "        return {\"wav\": waveform, \"target\": target}"
      ],
      "metadata": {
        "id": "B0pi-5JScUm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence \n",
        "\n",
        "def wav_collate_fn(data):\n",
        "    \"\"\"\n",
        "       data: is a list of tuples with (example, label, length)\n",
        "             where 'example' is a tensor of arbitrary shape\n",
        "             and label/length are scalars\n",
        "    \"\"\"\n",
        "    features = [torch.tensor(d['wav']) for d in data] #(3)\n",
        "    labels = torch.tensor([d['target']  for d in data]) \n",
        "    new_features = pad_sequence([f.T for f in features], batch_first=True).squeeze()\n",
        "\n",
        "    return  {\n",
        "        'wav': new_features.to(device), \n",
        "        'target': labels.to(device)\n",
        "    }"
      ],
      "metadata": {
        "id": "1UiwGyuIPZny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = VCC2018MOSWavDatasetLoad(audio_names_train, categories_train)\n",
        "loader_train = DataLoader(dataset_train, batch_size=15, shuffle=True, collate_fn=wav_collate_fn)\n",
        "\n",
        "dataset_test = VCC2018MOSWavDatasetLoad(audio_names_test, categories_test)\n",
        "loader_test = DataLoader(dataset_test, batch_size=2, shuffle=True, collate_fn=wav_collate_fn)\n"
      ],
      "metadata": {
        "id": "ZsozAAFGPeQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(loader_test):\n",
        "    print(type(batch['wav']))\n",
        "    print(batch['wav'].shape)\n",
        "    print(batch['wav'].get_device())\n",
        "    print(type(batch['target']))\n",
        "    print(batch['target'].shape)    \n",
        "    print(batch['target'].get_device())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z76XeYbY0n_I",
        "outputId": "34bb0702-f495-49e3-bd47-7df109718eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([2, 40652])\n",
            "0\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([2])\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, batch in enumerate(loader_train):\n",
        "    print(type(batch['wav']))\n",
        "    print(batch['wav'].shape)\n",
        "    print(batch['wav'].get_device())\n",
        "    print(type(batch['target']))\n",
        "    print(batch['target'].shape)    \n",
        "    print(batch['target'].get_device())\n",
        "    break\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kGvrhvWPkw8",
        "outputId": "71276b27-ed81-46f0-b083-4e178410e06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([15, 84722])\n",
            "0\n",
            "<class 'torch.Tensor'>\n",
            "torch.Size([15])\n",
            "0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_spec = dataset_train[10]['wav']\n",
        "one_label = dataset_train[10]['target']"
      ],
      "metadata": {
        "id": "ZHKUXMQ8iPTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one_spec = torch.tensor(one_spec, dtype=torch.int32)\n",
        "one_spec = one_spec.cpu().numpy() "
      ],
      "metadata": {
        "id": "S_n8gYraqaVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_spec.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za1m6q-ct872",
        "outputId": "99df12cd-696e-4c20-9cd9-c8e431205f0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 99200)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urFnaf8_gbl3",
        "outputId": "c6f90c6b-dc95-48aa-9861-8c54f0f2866a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
        "\n",
        "model_name_or_path = \"facebook/wav2vec2-base-960h\"\n",
        "pooling_mode = \"mean\"\n",
        "\n",
        "label_list = categories_types\n",
        "config = AutoConfig.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels=len(categories_types),\n",
        "    #label2id={label: i for i, label in enumerate(label_list)},\n",
        "    #id2label={i: label for i, label in enumerate(label_list)},\n",
        "    finetuning_task=\"wav2vec2_clf\",\n",
        ")"
      ],
      "metadata": {
        "id": "dhAevKDr8LCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.num_labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM2Sw40hTxr-",
        "outputId": "0aae2e83-27bf-46a6-8879-2192981509c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
        "    Wav2Vec2PreTrainedModel,\n",
        "    Wav2Vec2Model\n",
        ")\n",
        "\n",
        "class Wav2Vec2ClassificationHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.dropout = nn.Dropout(config.final_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "    def forward(self, features, **kwargs):\n",
        "        x = self.dropout(features)\n",
        "        x = self.dense(x)\n",
        "        x = self.dropout(torch.tanh(x))\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = 21\n",
        "        self.pooling_mode = nn.MaxPool2d(3)\n",
        "\n",
        "        self.wav2vec2 = Wav2Vec2Model(config)\n",
        "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def freeze_feature_extractor(self):\n",
        "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            input_values,\n",
        "            attention_mask=None\n",
        "    ):\n",
        "        outputs = self.wav2vec2(\n",
        "            input_values\n",
        "        )\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "        hidden_states = torch.sum(hidden_states, dim=1)\n",
        "        logits = self.classifier(hidden_states)\n",
        "\n",
        "        #return torch.flatten(logits)\n",
        "        return F.softmax(logits, 1)"
      ],
      "metadata": {
        "id": "Ao94ISaD8QZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content/VCC2018_MOS_preprocessed/wav')[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZtR3p7j8ZpU",
        "outputId": "6e21dca5-7951-4cdb-8eaa-86d23f3e509b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['N12_VCC2TF1_VCC2SM2_30002_HUB.wav',\n",
              " 'N19_VCC2TM2_VCC2SM1_30027_HUB.wav',\n",
              " 'N15_VCC2TF2_VCC2SF1_30020_HUB.wav',\n",
              " 'N13_VCC2TM1_VCC2SM4_30028_SPO.wav',\n",
              " 'N04_VCC2TM2_VCC2SM4_30005_SPO.wav',\n",
              " 'S00_VCC2TFX_VCC2SM1_30012_NAT.wav',\n",
              " 'N09_VCC2TM1_VCC2SF2_30034_HUB.wav',\n",
              " 'D01_VCC2TM2_VCC2SF1_30016_HUB.wav',\n",
              " 'N03_VCC2TF2_VCC2SF3_30034_SPO.wav',\n",
              " 'N04_VCC2TM2_VCC2SM1_30009_HUB.wav']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wav2vec_classifier = Wav2Vec2ForSpeechClassification(config)\n",
        "wav2vec_classifier = wav2vec_classifier.to(device)"
      ],
      "metadata": {
        "id": "sHHQ0Kja8Ud6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex_audio, sr = librosa.load('/content/VCC2018_MOS_preprocessed/wav/D03_VCC2TM1_VCC2SF1_30030_HUB.wav')\n",
        "ex_audio.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bEPQs7C8Yoq",
        "outputId": "93d6f645-9522-4b9b-c2e5-90eddf1e9cdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(115089,)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex_audio = torch.Tensor(ex_audio).unsqueeze(0)"
      ],
      "metadata": {
        "id": "gmO90top8wNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex_audio.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fS5bKDqHwcbB",
        "outputId": "a4adc9da-d1d3-46a5-9394-6e51561bd31c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 115089])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = wav2vec_classifier(ex_audio.to(device))\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEv1HXzy4AY0",
        "outputId": "8c8e4cda-94cf-490b-a850-d5ac911eb3aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 21])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5_8xrd8tloH",
        "outputId": "c087e5b4-91dc-41e1-9baa-347efc4df36c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 21])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(output, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMmngXsMtruo",
        "outputId": "1a23656b-0857-4201-cf9b-fa14395afe1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0453, 0.0922, 0.0948, 0.0489, 0.0613, 0.0290, 0.0271, 0.0425, 0.0646,\n",
              "         0.0136, 0.0354, 0.0263, 0.0269, 0.0607, 0.0276, 0.0410, 0.0733, 0.0407,\n",
              "         0.0360, 0.0555, 0.0571]], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = batch['wav']\n",
        "tmp.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsOEs41um9Qq",
        "outputId": "d6618f27-4c68-48f4-ce2a-720d1d319d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 64881])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = wav2vec_classifier(tmp)\n",
        "output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8pyQNr08nzX",
        "outputId": "51eda75f-a658-4cd1-9e62-8d13840fcd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 21])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(loader_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhZGxBGxlWe0",
        "outputId": "68145736-cc4c-4d44-89db-aca2aaa9126c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6174"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "WRhMMAVCQyTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "from IPython.display import clear_output\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "G5N5izq-QzMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion, scheduler, epoch=0):\n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss, accuracy, f1, recall, precision = 0, 0, 0, 0, 0 \n",
        "    total_steps = len(iterator)\n",
        "    for i, batch in enumerate(iterator):\n",
        "        specs = batch['wav'].to(device, dtype=torch.float32)\n",
        "        labels = batch['target'].to(device)\n",
        "\n",
        "        #specs = specs.unsqueeze(0)\n",
        "        #print(specs.shape)\n",
        "        #print(labels.shape)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(specs)\n",
        "        #print(output.shape)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "        #calculate metrics\n",
        "        #output = F.softmax(output, 1)\n",
        "        result = output.argmax(1)\n",
        "            \n",
        "        accuracy += accuracy_score(result.cpu(), labels.cpu())\n",
        "        f1 += f1_score(result.cpu(), labels.cpu(), average='micro')\n",
        "        recall += recall_score(result.cpu(), labels.cpu(), average='micro')\n",
        "        precision += precision_score(result.cpu(), labels.cpu(), average='micro')\n",
        "\n",
        "        if i % ( int(total_steps / 100)) == 0:\n",
        "          #step_loss = epoch_loss / (i+1)\n",
        "          #step_acc = accuracy / (i+1)\n",
        "          #step_f1 = f1 / (i+1)\n",
        "          #step_recall = recall / (i+1)\n",
        "          #step_precision = precision / (i+1)\n",
        "          #print(\"Train step {0}  loss: {1:.5f} acc: {2:.5f} f1: {3:.5f} recall {4:.5f} precision: {5:.5f}\".format(i, step_loss, step_acc, step_f1, step_recall, step_precision))\n",
        "          acc = accuracy_score(result.cpu(), labels.cpu())\n",
        "          print(\"Train step {0} / {1}  loss: {2:.5f} acc: {3:.5f}\".format(i, total_steps, loss.item(), acc))\n",
        "    #wandb.log({\"loss_train\": epoch_loss / (i+1), \"accuracy_train\": accuracy / (i+1), \"f1_train\": f1 / (i+1), \"recall_train\": recall / (i+1), \"precision_train\": precision / (i+1)})\n",
        "\n",
        "    #accuracy /= (i+1)\n",
        "    #f1 /= (i+1)\n",
        "    #epoch_loss /= (i+1)\n",
        "    #recall /= (i+1)\n",
        "    #precision /= (i+1)\n",
        "    accuracy /= total_steps\n",
        "    epoch_loss /= total_steps\n",
        "    recall /= total_steps\n",
        "    precision /= total_steps\n",
        "\n",
        "    return epoch_loss, accuracy, f1, recall, precision\n",
        "\n",
        "def evaluate(model, iterator, criterion, epoch):\n",
        "    model.eval()\n",
        "    epoch_loss, accuracy, f1, recall, precision = 0, 0, 0, 0, 0 \n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(iterator):\n",
        "            specs = batch['wav'].to(device, dtype=torch.float32)\n",
        "            batch_size = specs.shape[0]\n",
        "            \n",
        "            labels = batch['target'].to(device)\n",
        "            output = model(specs)\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            result = output.argmax(1)\n",
        "            \n",
        "            accuracy += accuracy_score(result.cpu(), labels.cpu())\n",
        "            f1 += f1_score(result.cpu(), labels.cpu(), average='micro')\n",
        "            recall += recall_score(result.cpu(), labels.cpu(), average='micro')\n",
        "            precision += precision_score(result.cpu(), labels.cpu(), average='micro')\n",
        "        \n",
        "    #wandb.log({\"loss_train\": epoch_loss / (i+1), \"accuracy_train\": accuracy / (i+1), \"f1_train\": f1 / (i+1), \"recall_train\": recall / (i+1), \"precision_train\": precision / (i+1)})\n",
        "\n",
        "    accuracy /= (i+1)\n",
        "    f1 /= (i+1)\n",
        "    epoch_loss /= (i+1)\n",
        "    recall /= (i+1)\n",
        "    precision /= (i+1)\n",
        "    \n",
        "    return epoch_loss, accuracy, f1, recall, precision\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "u4_aHktKQzsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init(project='emble_audio_classification', entity='miana')\n",
        "# config = wandb.config\n",
        "\n",
        "# change model below\n",
        "# model = CNNNetwork().to(device)\n",
        "model = wav2vec_classifier.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "lambda2 = lambda epoch: epoch * 0.95\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,  lr_lambda=[lambda2])"
      ],
      "metadata": {
        "id": "faMnnIDpQ003"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 10\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "train_accuracies, val_accuracies = [], []\n",
        "train_f1s, val_f1s = [], []\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "#wandb.watch(model)\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss, train_accuracy, train_f1, train_recall, train_precision = train(model, loader_train, optimizer, criterion, scheduler, epoch)\n",
        "    val_loss, val_accuracy, val_f1, val_recall, val_precision = evaluate(model, loader_test, criterion, epoch)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    # fill data\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    train_f1s.append(train_f1)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "    val_f1s.append(val_f1)\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    torch.save(model.state_dict(), 'best-val-model.pt')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins} m {epoch_secs} s')\n",
        "    print(f'\\tTrain Loss: {train_loss}, accuracy: {train_accuracy}, f1 {train_f1}, recall {train_recall}, precision {train_precision}')\n",
        "    print(f'\\t Val. Loss: {val_loss}, accuracy: {val_accuracy}, f1 {val_f1}, recall {val_recall}, precision {val_precision}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Oi9CYsBJRB-G",
        "outputId": "d9a7f7ea-8d16-48fa-e234-6cf38329951e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train step 0 / 1235  loss: 3.04410 acc: 0.06667\n",
            "Train step 12 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 24 / 1235  loss: 2.98984 acc: 0.13333\n",
            "Train step 36 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 48 / 1235  loss: 2.98984 acc: 0.13333\n",
            "Train step 60 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 72 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 84 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 96 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 108 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 120 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 132 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 144 / 1235  loss: 2.98984 acc: 0.13333\n",
            "Train step 156 / 1235  loss: 2.98984 acc: 0.13333\n",
            "Train step 168 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 180 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 192 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 204 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 216 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 228 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 240 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 252 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 264 / 1235  loss: 2.98984 acc: 0.13333\n",
            "Train step 276 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 288 / 1235  loss: 2.98984 acc: 0.13333\n",
            "Train step 300 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 312 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 324 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 336 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 348 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 360 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 372 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 384 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 396 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 408 / 1235  loss: 2.98984 acc: 0.13333\n",
            "Train step 420 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 432 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 444 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 456 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 468 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 480 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 492 / 1235  loss: 2.98984 acc: 0.13333\n",
            "Train step 504 / 1235  loss: 2.98984 acc: 0.13333\n",
            "Train step 516 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 528 / 1235  loss: 2.92317 acc: 0.20000\n",
            "Train step 540 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 552 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 564 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 576 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 588 / 1235  loss: 2.92317 acc: 0.20000\n",
            "Train step 600 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 612 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 624 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 636 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 648 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 660 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 672 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 684 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 696 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 708 / 1235  loss: 2.92317 acc: 0.20000\n",
            "Train step 720 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 732 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 744 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 756 / 1235  loss: 2.92317 acc: 0.20000\n",
            "Train step 768 / 1235  loss: 2.98984 acc: 0.13333\n",
            "Train step 780 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 792 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 804 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 816 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 828 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 840 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 852 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 864 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 876 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 888 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 900 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 912 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 924 / 1235  loss: 2.98984 acc: 0.13333\n",
            "Train step 936 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 948 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 960 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 972 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 984 / 1235  loss: 2.85650 acc: 0.26667\n",
            "Train step 996 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 1008 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 1020 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 1032 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 1044 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 1056 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 1068 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 1080 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 1092 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 1104 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 1116 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 1128 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 1140 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 1152 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 1164 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 1176 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 1188 / 1235  loss: 3.12317 acc: 0.00000\n",
            "Train step 1200 / 1235  loss: 2.92317 acc: 0.20000\n",
            "Train step 1212 / 1235  loss: 3.05650 acc: 0.06667\n",
            "Train step 1224 / 1235  loss: 3.12317 acc: 0.00000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-ec7305d49515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-ee4b7098dd2a>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, iterator, criterion, epoch)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RFgQtMrcUIEz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}