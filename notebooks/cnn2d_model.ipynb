{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "519c8186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "817fba0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92750c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence \n",
    "def custom_spec_collate_fn(data):\n",
    "    \"\"\"\n",
    "       data: is a list of tuples with (example, label, length)\n",
    "             where 'example' is a tensor of arbitrary shape\n",
    "             and label/length are scalars\n",
    "    \"\"\"\n",
    "    features = [torch.tensor(d['data']) for d in data] #(3)\n",
    "    labels = torch.tensor([d['target']  for d in data]) \n",
    "    new_features = pad_sequence([f.T for f in features], batch_first=True).squeeze()\n",
    "\n",
    "    return  {\n",
    "        'data': new_features.to(device), \n",
    "        'target': labels.to(device)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1a16416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VCC2018SpecDatasetLoad(Dataset):\n",
    "    def __init__(self, audio_names: list, labels: list):\n",
    "        self.audio_names = audio_names\n",
    "        self.labels = labels\n",
    "        self.transformation = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=16000,\n",
    "            n_fft=1024,\n",
    "            hop_length=512,\n",
    "            n_mels=80\n",
    "        )\n",
    "        #self.label_to_id = dict((mos,id) for id, mos in enumerate(labels))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.audio_names[idx]\n",
    "        #waveform, sample_rate = torchaudio.load(filename)\n",
    "        wav, sr = torchaudio.load(filename)\n",
    "        mel_spec = self.transformation(wav).unsqueeze(1)\n",
    "\n",
    "        target = self.labels[idx]\n",
    "        \n",
    "        return {\"data\": mel_spec, \"target\": target}\n",
    "\n",
    "    def get_transform_spec(self, idx, data_dir='./'):\n",
    "        # get audio path\n",
    "        specs = []\n",
    "        audio_name = self.audio_names[idx]\n",
    "        audio_path = os.path.join(data_dir, audio_name)\n",
    "          \n",
    "        # load audio and get its melspectrogram\n",
    "        audio_wave, sr = librosa.load(audio_path, sr=None)\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio_wave, sr=sr)\n",
    "        \n",
    "        mel_spec = librosa.power_to_db(mel_spec)\n",
    "        mel_spec = torch.Tensor(self.normalize(mel_spec)).unsqueeze(0)\n",
    "        return mel_spec      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c71f56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a13e93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.zeros_(m.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.avg_pool2d(x, 2)\n",
    "        return x\n",
    "\n",
    "class CNN2D_MODEL(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            ConvBlock(in_channels=1, out_channels=64),\n",
    "            ConvBlock(in_channels=64, out_channels=128),\n",
    "            ConvBlock(in_channels=128, out_channels=256),\n",
    "            ConvBlock(in_channels=256, out_channels=512),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.PReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        x, _ = torch.max(x, dim=2)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1948fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = '../data/train.csv'\n",
    "test_csv = '../data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb31f168",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(train_csv)\n",
    "test_data = pd.read_csv(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "176a63d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        VCC2018_MOS_preprocessed/wav/N09_VCC2TF1_VCC2S...\n",
       "1        VCC2018_MOS_preprocessed/wav/N11_VCC2TF2_VCC2S...\n",
       "2        VCC2018_MOS_preprocessed/wav/N16_VCC2TM1_VCC2S...\n",
       "3        VCC2018_MOS_preprocessed/wav/N04_VCC2TM1_VCC2S...\n",
       "4        VCC2018_MOS_preprocessed/wav/N11_VCC2TF1_VCC2S...\n",
       "                               ...                        \n",
       "18517    VCC2018_MOS_preprocessed/wav/D04_VCC2TF1_VCC2S...\n",
       "18518    VCC2018_MOS_preprocessed/wav/N04_VCC2TF2_VCC2S...\n",
       "18519    VCC2018_MOS_preprocessed/wav/N12_VCC2TF2_VCC2S...\n",
       "18520    VCC2018_MOS_preprocessed/wav/N18_VCC2TM2_VCC2S...\n",
       "18521    VCC2018_MOS_preprocessed/wav/D05_VCC2TM2_VCC2S...\n",
       "Name: filepath, Length: 18522, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['filepath'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44494da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['filepath'] = '../../../DATASETS/MOS/' + train_data['filepath']\n",
    "test_data['filepath'] = '../../../DATASETS/MOS/' + test_data['filepath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1dafc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_train = train_data['score'].to_list()\n",
    "audio_names_train = train_data['filepath'].to_list()\n",
    "\n",
    "categories_test = test_data['score'].to_list()\n",
    "audio_names_test = test_data['filepath'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2383f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(pd.concat([train_data, test_data])['score'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1dc27d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = VCC2018SpecDatasetLoad(audio_names_train, categories_train)\n",
    "loader_train = DataLoader(dataset_train, batch_size=48, shuffle=True, collate_fn=custom_spec_collate_fn)\n",
    "\n",
    "dataset_test = VCC2018SpecDatasetLoad(audio_names_test, categories_test)\n",
    "loader_test = DataLoader(dataset_test, batch_size=8, shuffle=True, collate_fn=custom_spec_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "680982ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN2D_MODEL(num_classes=num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f753372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, scheduler, epoch=0):\n",
    "    model.train()\n",
    "    epoch_loss, accuracy, f1, recall, precision = 0, 0, 0, 0, 0\n",
    "\n",
    "    total_steps = len(iterator)\n",
    "    for i, batch in enumerate(iterator):\n",
    "        data = batch['data'].to(device, dtype=torch.float32).unsqueeze(1)\n",
    "        labels = batch['target'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # calculate metrics\n",
    "        output = F.softmax(output, 1)\n",
    "        result = output.argmax(1)\n",
    "\n",
    "        accuracy += accuracy_score(result.cpu(), labels.cpu())\n",
    "        f1 += f1_score(result.cpu(), labels.cpu(), average='micro')\n",
    "        recall += recall_score(result.cpu(), labels.cpu(), average='micro')\n",
    "        precision += precision_score(result.cpu(), labels.cpu(), average='micro')\n",
    "\n",
    "        if (i % 100 == 0):\n",
    "            step_loss = \"{:.5f}\".format(epoch_loss / (i + 1))\n",
    "            step_acc = \"{:.5f}\".format(accuracy / (i + 1))\n",
    "            step_f1 = \"{:.5f}\".format(f1 / (i + 1))\n",
    "            step_recall = \"{:.5f}\".format(recall / (i + 1))\n",
    "            step_precision = \"{:.5f}\".format(precision / (i + 1))\n",
    "            print(\n",
    "                f\"Train step {i} loss: {step_loss} acc: {step_acc} f1: {step_f1} recall {step_recall} precision: {step_precision}\")\n",
    "    # wandb.log({\"loss_train\": epoch_loss / (i+1), \"accuracy_train\": accuracy / (i+1), \"f1_train\": f1 / (i+1), \"recall_train\": recall / (i+1), \"precision_train\": precision / (i+1)})\n",
    "\n",
    "    accuracy /= (i + 1)\n",
    "    f1 /= (i + 1)\n",
    "    epoch_loss /= (i + 1)\n",
    "    recall /= (i + 1)\n",
    "    precision /= (i + 1)\n",
    "\n",
    "    return epoch_loss, accuracy, f1, recall, precision\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, epoch):\n",
    "    model.eval()\n",
    "    epoch_loss, accuracy, f1, recall, precision = 0, 0, 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            data = batch['data'].to(device, dtype=torch.float32).unsqueeze(1)\n",
    "            batch_size = data.shape[0]\n",
    "\n",
    "            labels = batch['target'].to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            result = output.argmax(1)\n",
    "\n",
    "            accuracy += accuracy_score(result.cpu(), labels.cpu())\n",
    "            f1 += f1_score(result.cpu(), labels.cpu(), average='micro')\n",
    "            recall += recall_score(result.cpu(), labels.cpu(), average='micro')\n",
    "            precision += precision_score(result.cpu(), labels.cpu(), average='micro')\n",
    "\n",
    "    # wandb.log({\"loss_train\": epoch_loss / (i+1), \"accuracy_train\": accuracy / (i+1), \"f1_train\": f1 / (i+1), \"recall_train\": recall / (i+1), \"precision_train\": precision / (i+1)})\n",
    "\n",
    "    accuracy /= (i + 1)\n",
    "    f1 /= (i + 1)\n",
    "    epoch_loss /= (i + 1)\n",
    "    recall /= (i + 1)\n",
    "    precision /= (i + 1)\n",
    "\n",
    "    return epoch_loss, accuracy, f1, recall, precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8dbc375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda2 = lambda epoch: epoch * 0.95\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,  lr_lambda=[lambda2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7792364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8330d8f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step 0 loss: 3.16676 acc: 0.08333 f1: 0.08333 recall 0.08333 precision: 0.08333\n",
      "Train step 100 loss: 3.21965 acc: 0.03837 f1: 0.03837 recall 0.03837 precision: 0.03837\n",
      "Train step 200 loss: 3.20451 acc: 0.04239 f1: 0.04239 recall 0.04239 precision: 0.04239\n",
      "Train step 300 loss: 3.20255 acc: 0.04298 f1: 0.04298 recall 0.04298 precision: 0.04298\n",
      "Epoch: 01\n",
      "\tTrain Loss: 3.2022181174915687, accuracy: 0.04302368615840121, f1 0.04302368615840121, recall 0.04302368615840121, precision 0.04302368615840121\n",
      "\t Val. Loss: 3.096637265626774, accuracy: 0.04844961240310078, f1 0.04844961240310078, recall 0.04844961240310078, precision 0.04844961240310078\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "train_f1s, val_f1s = [], []\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# wandb.watch(model)\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_accuracy, train_f1, train_recall, train_precision = train(model, loader_train, optimizer,\n",
    "                                                                                criterion, scheduler, epoch)\n",
    "    val_loss, val_accuracy, val_f1, val_recall, val_precision = evaluate(model, loader_test, criterion, epoch)\n",
    "\n",
    "\n",
    "    # fill data\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_f1s.append(train_f1)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_f1s.append(val_f1)\n",
    "\n",
    "    torch.save(model.state_dict(), 'best-val-model.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02}')\n",
    "    print(\n",
    "        f'\\tTrain Loss: {train_loss}, accuracy: {train_accuracy}, f1 {train_f1}, recall {train_recall}, precision {train_precision}')\n",
    "    print(\n",
    "        f'\\t Val. Loss: {val_loss}, accuracy: {val_accuracy}, f1 {val_f1}, recall {val_recall}, precision {val_precision}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1298d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
